{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZcHtd7jmciqEnznvyEVbu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/linhoangce/tensorflow_learning_stuff/blob/main/decision_forests.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Migrating from NN\n",
        "\n",
        "Decision Forests work differently than Neural Networks (NN): DFs generally do not train with backpropagation, or in mini-batches. Therefore, TF-DF pipelines have a few differences from other Tensorflow pipelines.\n"
      ],
      "metadata": {
        "id": "ClZ_GGA6WI61"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset and Features\n",
        "\n",
        "### Validation Dataset\n",
        "\n",
        "Unlike the standar Neural Network training paradigm, TF-DF models do not need a validation dataset to monior overfitting, or to stop training early. If we already have a train/validation/test split, and we are using the validation for one of those reason, it is safe to train our TF-DF on train+validation (ubless the validation split is also used for something else, like hyper-parameter tuning)\n",
        "\n",
        "\n",
        "* NN:\n",
        "<code>\n",
        "\n",
        "model.fit(train_ds, validation_data=val_ds)\n",
        "\n",
        "</code>\n",
        "\n",
        "Or just combine both and no need for validation dataset\n",
        "\n",
        "* DF:\n",
        "<code>\n",
        "\n",
        "model.fit(train_ds.concatenate(val_ds))\n",
        "\n",
        "</code>\n",
        "\n",
        "\n",
        "**Rationale**: the TF-DF framework is composed of multiple algorithms. Some of them do not use a validation dataset (e.g Random Forest) while some others do (e.g Gradient Boosted Trees). Algorithms that do might benefit from different types and size of validation datasets. Therefore, if a valiation dataset is needed, it will be extracted automatically from the training dataset.\n",
        "\n",
        "### Dataset I/O\n",
        "\n",
        "**traing for exactly 1 epoch**"
      ],
      "metadata": {
        "id": "8rVrsx0SWnKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_decision_forests as tfdf\n",
        "\n",
        "model = tfdf.keras.RandomForestModel()\n",
        "train_ds = ...\n",
        "### NN\n",
        "# Numer of epochs in Keras\n",
        "model.fit(train_ds, num_epcohs=5)\n",
        "\n",
        "# Number of epochs in the dataset\n",
        "train_ds = train_ds.repeat(5)\n",
        "model.fit(train_ds)\n",
        "\n",
        "\n",
        "### DF\n",
        "model.fit(train_ds)"
      ],
      "metadata": {
        "id": "ZYAKrECsXbAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rationale**: Users of neural networks often train a model for N steps (which may involve looping over the dataset > 1 time), due to the nature of SGD. TF-DF trains by reading the whole dataset and then running the training at the end. 1 epoch is needed to read the full dataset, and any extra steps will result in unncessary data I/O, as well as slower training."
      ],
      "metadata": {
        "id": "h0_k6GOgYvcq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Do not shuffle the dataset**\n",
        "\n",
        "Datasets do not need to be shuffled (unless the unput_fn is reading only a sample of the dataset)\n",
        "\n",
        "**Rationale**: TF-DF shuffles access to the data internally after reading the full dataset into memory. TF-DF algorithms are deterministic (if the user does not change the random seed). Enabling shuffling will only make the algorithm non-deterministic. Shuffling does make sense if the input dataset is ordered and the input_fn is only going to read a sample of it (the sample should be random). However, this will make the training procedure non-deterministic."
      ],
      "metadata": {
        "id": "X-IVNUbIZT1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = train_ds.shuffle(5)\n",
        "model.fit(train_ds)\n",
        "\n",
        "### DF\n",
        "model.fit(train_ds)"
      ],
      "metadata": {
        "id": "LG4igNAtZiXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Do not tune the batch size**\n",
        "\n"
      ],
      "metadata": {
        "id": "7lSQVwOAac4e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Large Datasets\n",
        "\n",
        "Unlike NN, which can loop over mini-batches of a large dataset infinitely,  DF requires a finite dataset that fits in memorr for their training procedures. The size of the dataset has performance and memory implications.\n",
        "\n",
        "There are diminishing returns for increasing the size of the dataset, and DF-algorithms arguably need fewer examples for convergence than large NN models. Instead of scaling the number of training steps (as in a NN), we can try scaling the amount of data to see where the compute tradeoff makes sense. Therefore, **it is a good idea to first try training on a (small) subset of the dataset.**\n",
        "\n",
        "The alternative solution is to use *distributed training*. Distributed training is great way to increase the size of the dataset if multiple machines are available. While all the distributed algorimths are available to distribute the computation, not all of them are able to distribute the RAM usage.\n",
        "\n",
        "**How many examples to use**\n",
        "\n",
        "**It should fit in memory on the machine the model is training on:**\n",
        "\n",
        "* Note that this is not the same as the size of the examples on disk\n",
        "\n",
        "* as a rule of thumn one numerical or categorical value uses **4 bytes** of memory. So, a dataset with 100 features and 25 million examples will take ~10GB (= 100 * 25 * 10^6 * 4) of memory.\n",
        "\n",
        "* Categorical-set features (e.g tokenized text) take more memory (4 bytes per token + 12 bytes per feature)\n",
        "\n",
        "** Consider what training time budget is**\n",
        "\n",
        "* while generally faster than NN for smaller datasets (e.g < 100k examples), DF training algorithms do no scale linearly with the dataset size; rather, ~O(features x num_examples x log(num_examples)) in most cases.\n",
        "\n",
        "* The training time depends on the hyperparameters. The most impactful parameters are: (1) the number of trees (`num_tree`), (2) the example sampling rate (`subsample` for GBT), (3) the attribute sampling rate (`num_candidate_attribute_ratio`)\n",
        "\n",
        "* Categorical-set features are more expensive than other features. The cost is controlled by the `categorical_set_split_greedy_sampling` parameter.\n",
        "\n",
        "* Sparse Oblique features (disabled by default) give good results but are expensive to compute.\n",
        "\n",
        "**Rules of thumb for scaling up data**\n",
        "\n",
        "It is suggested to start with a small slice of the data (< 10k examples), which should allow for training a TF-DF model in seconds or a few minutes in most case. Then we can increase the data at a fixed rate (e.g %40 more each time), stopping when validation set performance does not improve or the dataset no longer fits in memory."
      ],
      "metadata": {
        "id": "JKzmSx8_ai-L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Determinism\n",
        "\n",
        "The TF-DF training algorithm is deterministic, i.e training twice on the same dataset will give the exact same model. This is different from neural networks trained with TensorFlow. To preserve this determinism, users should ensure that dataset reads are deterministic as well.\n",
        "\n",
        "### Training Configuration\n",
        "\n",
        "**Specify a task (e.g classification, ranking) instead of a loss (e.g binary cross-entropy)**"
      ],
      "metadata": {
        "id": "xsKiU_cQl5_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### NN\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(1)) # One output for binary classification\n",
        "\n",
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer='Adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "### DF\n",
        "# the loss is automatically determined from the task\n",
        "model = tfdf.keras.GradientBoostedTreesModel(task=tf_keras.Task.CLASSIFICATION)\n",
        "\n",
        "# Optional if we want to report the accuracy\n",
        "model.compile(metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "W6Tw40dQmIN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hyperparemters are semantically stable**\n",
        "\n",
        "All the hyperparameters have default values. Those values are reasonable first candidates to try. Default hyperparameter values are guaranteed to never change. For this reason, new hyperpararmters of algorithm improvements are disabled by default.\n",
        "\n",
        "Users that wish to use the latest algorithms, but who do not want ot optimize the hyperparameters themselves can use the \"hyperparamters templates\" provided by TF-DF."
      ],
      "metadata": {
        "id": "fUoGVl0_nW8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model with default hyperparameters\n",
        "model = tfdf.keras.GradientBoostedTreesModel()\n",
        "\n",
        "# List the hyperparameters (with default values)\n",
        "# and hyperparameters templates of the GBT learning algo\n",
        "?tfdf.keras.GradientBoostedTreesModel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yk1WW4Jan4mu",
        "outputId": "03ca70f9-ade4-4695-c62c-36ff238d5f35"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use /tmp/tmpb49rf7sf as temporary training directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a hyperparameter template\n",
        "model = tfdf.keras.GradientBoostedTreesModel(hyperparameter_template='better_default')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5204RgzMoOM3",
        "outputId": "ede2eff7-861d-4927-af00-89868da20a74"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resolve hyper-parameter template \"better_default\" to \"better_default@v1\" -> {'growing_strategy': 'BEST_FIRST_GLOBAL'}.\n",
            "Use /tmp/tmpq0icqrzk as temporary training directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change one of the hyperparameters\n",
        "model = tfdf.keras.GradientBoostedTreesModel(num_trees=500)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWlSxntQopWN",
        "outputId": "61a8f3cc-386e-4f76-a4c2-2dcc99e635d4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use /tmp/tmp8aibunee as temporary training directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List all the learning algorithms available\n",
        "tfdf.keras.get_all_models()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCn-FNHDo9Kh",
        "outputId": "8b9f6d78-954b-453b-ee80-f19d03da79bd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensorflow_decision_forests.keras.RandomForestModel,\n",
              " tensorflow_decision_forests.keras.GradientBoostedTreesModel,\n",
              " tensorflow_decision_forests.keras.CartModel,\n",
              " tensorflow_decision_forests.keras.DistributedGradientBoostedTreesModel]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model debugging"
      ],
      "metadata": {
        "id": "hA7--hcEpCk_"
      }
    }
  ]
}